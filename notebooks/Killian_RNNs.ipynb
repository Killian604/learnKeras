{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "340c411000d38f9d7e7cbfee7daf46d49571960a"
   },
   "source": [
    "### Aim: practice using RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f82cc24efc6283e5a98cadb78fc1a465e44c143",
    "collapsed": true
   },
   "source": [
    "# LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "da0236e4b36ce514c1fec3fd72f236d1fa259131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session setup successful\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, GRU, LSTM  # Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.backend import tensorflow_backend as tb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime as dt\n",
    "import IPython\n",
    "import keras as k\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "plt.style.use('fivethirtyeight')\n",
    "print(\"Session setup successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradual memory alloc setup successful!\n"
     ]
    }
   ],
   "source": [
    "# Allow gradual memory alloc for GPU use\n",
    "random_seed = 1 # Random seed included for reproducability/consistency reasons when learning.\n",
    "np.random.seed(random_seed)\n",
    "if tb._SESSION is None:\n",
    "    if not os.environ.get('OMP_NUM_THREADS'):\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    else:\n",
    "        num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=num_thread, allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth=True\n",
    "    _SESSION = tf.Session(config=config)\n",
    "session = _SESSION\n",
    "print('Gradual memory alloc setup successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to help out with\n",
    "def plot_predictions(test,predicted,stock='IBM') -> None:\n",
    "    plt.plot(test, color='red',label='Real IBM Stock Price')\n",
    "    plt.plot(predicted, color='blue',label='Predicted IBM Stock Price')\n",
    "    plt.title('IBM Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(stock + ' Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def return_rmse(test,predicted) -> None:\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AABA_2006-01-01_to_2018-01-01.csv',\n",
       " 'AAPL_2006-01-01_to_2018-01-01.csv',\n",
       " 'all_stocks_2006-01-01_to_2018-01-01.csv',\n",
       " 'all_stocks_2017-01-01_to_2018-01-01.csv',\n",
       " 'AMZN_2006-01-01_to_2018-01-01.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_path = '../data/stock-time-series-20050101-to-20171231/'\n",
    "os.listdir(stock_data_path)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4cf10cf27420eb383b93b15c0895139ea96c0ed3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in dataframe: 3020\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-01-03</th>\n",
       "      <td>82.45</td>\n",
       "      <td>82.55</td>\n",
       "      <td>80.81</td>\n",
       "      <td>82.06</td>\n",
       "      <td>11715200</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-04</th>\n",
       "      <td>82.20</td>\n",
       "      <td>82.50</td>\n",
       "      <td>81.33</td>\n",
       "      <td>81.95</td>\n",
       "      <td>9840600</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-05</th>\n",
       "      <td>81.40</td>\n",
       "      <td>82.90</td>\n",
       "      <td>81.00</td>\n",
       "      <td>82.50</td>\n",
       "      <td>7213500</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close    Volume Name\n",
       "Date                                                 \n",
       "2006-01-03  82.45  82.55  80.81  82.06  11715200  IBM\n",
       "2006-01-04  82.20  82.50  81.33  81.95   9840600  IBM\n",
       "2006-01-05  81.40  82.90  81.00  82.50   7213500  IBM"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we get the data\n",
    "dataset = pd.read_csv(stock_data_path + 'IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\n",
    "print(\"Number of entries in dataframe:\", len(dataset))\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Name'], dtype='object')\n",
      "Index(['High', 'Low', 'Close', 'Volume'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataset.columns[:])\n",
    "print(dataset.columns[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "fb4c9db6d8a5bcf20ffad41747cfa5b6215ba220"
   },
   "outputs": [],
   "source": [
    "# # Siddarth: Checking for missing values (K: ??? This doesn't do any null checking AFAIK!)\n",
    "# training_set = dataset[:'2016'].iloc[:,3:4].values\n",
    "# test_set = dataset['2017':].iloc[:,3:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-22fae982261a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    "training_set[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # PROJECT PARAMETERS # # #\n",
    "features = ['High', 'Low', 'Close']\n",
    "num_prev_elems = 60\n",
    "min_max_scale = (0.05, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = dataset[:'2016'].loc[:, features].values\n",
    "test_set = dataset['2017':].loc[:, features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf5a9463d58e73852d2b70be9611e8cf1f4166fd"
   },
   "outputs": [],
   "source": [
    "dataset[\"Close\"][:'2016'].plot(figsize=(16,4),legend=True)\n",
    "dataset[\"Close\"]['2017':].plot(figsize=(16,4),legend=True)\n",
    "plt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\n",
    "plt.title('IBM stock price (close)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc9c36165fc07d258bd5ea87874d2da17fa4a4d"
   },
   "outputs": [],
   "source": [
    "# Scaling the training set\n",
    "sc = MinMaxScaler(feature_range=min_max_scale)\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "training_set_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fccfb866a2b4c702e0b2742f7c0289512d713d1b"
   },
   "outputs": [],
   "source": [
    "# set up x_train, y_train\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(num_prev_elems, len(training_set)):  # training set is 2759 elems long\n",
    "    x_train.append(\n",
    "        training_set_scaled[i-num_prev_elems:i, :])\n",
    "    y_train.append(\n",
    "        training_set_scaled[i,:])\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "print('xtrain shape:', x_train.shape)\n",
    "print('ytrain shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "637f699d3c4bde4b783de56ed4dd70a1bf59760d"
   },
   "outputs": [],
   "source": [
    "# Reshaping x_train for efficient modelling\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))  # <--TODO: what happens here?!\n",
    "print('new xtrain shape:', x_train.shape)\n",
    "print(\"As seen above, RESHAPING IS USELESS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df20eb7e8062dae0a3aff2182aa440faddd0017d"
   },
   "outputs": [],
   "source": [
    "# Build LTSM model\n",
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "regressor.add(Dropout(0.5))\n",
    "regressor.add(LSTM(units=40, return_sequences=True))\n",
    "regressor.add(Dropout(0.5))\n",
    "# regressor.add(LSTM(units=30, return_sequences=True))\n",
    "# regressor.add(Dropout(0.5))\n",
    "# regressor.add(LSTM(units=20))\n",
    "# regressor.add(Dropout(0.5))\n",
    "# regressor.add(Flatten())\n",
    "regressor.add(Dense(units=1))\n",
    "# model variables\n",
    "ltsm_batch_size = 32\n",
    "ltsm_epochs = 5\n",
    "# Some other optimizers include: RMSprop,Adagrad,Adadelta,Adam\n",
    "ltsm_optimizer = k.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0) # Siddarth\n",
    "# ltsm_optimizer = k.optimizers.Adam()\n",
    "regressor.compile(optimizer=ltsm_optimizer, loss='mean_squared_error')  # TODO: figure out other loss functions. Hinge?\n",
    "ltsm_runtime_name = 'LTSM_504030_ADAM' \\\n",
    "    + '_BS' + str(ltsm_batch_size) \\\n",
    "    + '_epochs' + str(ltsm_epochs) \\\n",
    "    + '_TensorboardStopEarly' \\\n",
    "    + '_' + str(dt.datetime.now()).replace(\":\",\"H\",1).replace(\":\",\"M\",1)\n",
    "# Declare callbacks  # Some metrics to MONITER include 'loss' and 'val_loss'\n",
    "stopearly = k.callbacks.EarlyStopping(monitor='loss', min_delta=0.0009, patience=1, verbose=1,\n",
    "                                      mode='auto', baseline=None, restore_best_weights=False)\n",
    "tensorboard_ltsm = k.callbacks.TensorBoard(log_dir='../tensorboard/' + ltsm_runtime_name,\n",
    "                                           histogram_freq=0,batch_size=ltsm_batch_size, write_graph=True, write_images=True)\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize model (Comment out as needed)\n",
    "k.utils.plot_model(regressor, to_file='../visualizations/' + ltsm_runtime_name +'.png', show_shapes=True)\n",
    "IPython.display.Image('../visualizations/' + ltsm_runtime_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = regressor.fit(x_train, y_train, epochs=ltsm_epochs, batch_size=ltsm_batch_size,  # shuffle=True,\n",
    "                        verbose=1, callbacks=[stopearly, tensorboard_ltsm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "326fa85615622feb484cc4c848edeec6f7133913"
   },
   "outputs": [],
   "source": [
    "# Prep test set similar to train set\n",
    "# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless\n",
    "# we take the whole \n",
    "# 'High' attribute data for processing\n",
    "dataset_total = pd.concat(\n",
    "    (dataset[\"Close\"][:'2016'],\n",
    "     dataset[\"Close\"]['2017':]),axis=0)\n",
    "print('len(datasettotal)', len(dataset_total))\n",
    "print('len(test_set)', len(test_set))\n",
    "print()\n",
    "# Get inputs as _\n",
    "inputs = dataset_total[ len(dataset_total) - len(test_set) - num_prev_elems: ].values\n",
    "print(\"Sample of inputs:\", inputs[:5])\n",
    "# Reshape inputs\n",
    "inputs = inputs.reshape(-1,1)\n",
    "print(\"Sample of inputs reshaped:\", inputs[:5])\n",
    "# Squishify inputs between 0 and 1\n",
    "inputs  = sc.transform(inputs)\n",
    "print(\"Samples of inputs transformed:\", inputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "435b8024814939ac4fbd372baa0cd8cfc78f80bc"
   },
   "outputs": [],
   "source": [
    "# Preparing X_test and predicting the prices\n",
    "X_test = []\n",
    "for i in range(num_prev_elems, 311):  # How did this value of 311 get calculated?\n",
    "    X_test.append(\n",
    "        inputs[i - num_prev_elems:i, 0]\n",
    "    )\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b774a8e79e53eac89694cafef6b11aa99226b95f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing the results for LSTM\n",
    "plot_predictions(test_set, predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6f6db0b6e1f17ac63c06ce49856873d98ba5f00"
   },
   "outputs": [],
   "source": [
    "# Evaluating our model\n",
    "return_rmse(test_set, predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4cf704ab3cd091f63b7b9a1b9224a49f0913171"
   },
   "source": [
    "---\n",
    "\n",
    "## Gated Recurrent Units\n",
    "In simple words, the GRU unit does not have to use a memory unit to control the flow of information like the LSTM unit. It can directly makes use of the all hidden states without any control. GRUs have fewer parameters and thus may train a bit faster or need less data to generalize. But, with large data, the LSTMs with higher expressiveness may lead to better results.\n",
    "\n",
    "They are almost similar to LSTMs except that they have two gates: reset gate and update gate. Reset gate determines how to combine new input to previous memory and update gate determines how much of the previous state to keep. Update gate in GRU is what input gate and forget gate were in LSTM. We don't have the second non linearity in GRU before calculating the outpu, .neither they have the output gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9b616c5112d707d16cc4b277007e286cffd58f6"
   },
   "outputs": [],
   "source": [
    "# The GRU architecture (original architecture has 20% dropout b/w layers)\n",
    "regressorGRU = Sequential()\n",
    "regressorGRU.add(GRU(units=45, return_sequences=True,\n",
    "                     input_shape=(x_train.shape[1],1),\n",
    "                     activation='tanh'))  # TODO: he specifies tanh function, but whats the default? other options are..?\n",
    "regressorGRU.add(GRU(units=30, return_sequences=True,  # TODO: why input shape thru all levels? necessary?\n",
    "                     input_shape=(x_train.shape[1],1),\n",
    "                     activation='tanh'))\n",
    "regressorGRU.add(GRU(units=20, return_sequences=True,\n",
    "                     input_shape=(x_train.shape[1],1),\n",
    "                     activation='tanh'))\n",
    "regressorGRU.add(GRU(units=10, activation='tanh'))\n",
    "regressorGRU.add(Dense(units=1))\n",
    "# Model variables\n",
    "gru_batch_size = 32 # keras.losses.hinge(y_true, y_pred)\n",
    "\n",
    "gru_epochs = 6\n",
    "# model_optimizer = SGD(lr=0.005, decay=1e-7, momentum=0.95, nesterov=False)  # Default\n",
    "# gru_optimizer = k.optimizers.Adadelta()  # First run of Adadelta was S L O W compared to SGD. Terrible error on a 3 epoch run. Not great.\n",
    "# gru_optimizer = k.optimizers.Adam() # Great first run. Small error, small lag behind actual data\n",
    "# gru_optimizer = k.optimizers.Adagrad()  # OK. Good adherence to small changes, but error larger than liked.\n",
    "gru_optimizer = k.optimizers.RMSprop() # Great! great adherence, low error. a good contender. \n",
    "# gru_optimizer = SGD()  # Standard out. Error normal, not bad but not great. \n",
    "\n",
    "regressorGRU.compile(optimizer=gru_optimizer, loss=k.losses.mean_squared_error)\n",
    "# Callbacks\n",
    "stopearly = k.callbacks.EarlyStopping(monitor='loss', min_delta=0.0009, patience=1, verbose=1,\n",
    "                                      mode='auto', baseline=None, restore_best_weights=False)\n",
    "gru_runtime_name = 'GRU_45302010_RMSPROP' \\\n",
    "+ '_BS' + str(gru_batch_size) \\\n",
    "+ '_epochs' + str(gru_epochs) \\\n",
    "+ '_TensorboardStopEarly' \\\n",
    "+ '_' + str(dt.datetime.now()).replace(\":\",\"H\",1).replace(\":\",\"M\",1)\n",
    "tensorboard_gru = k.callbacks.TensorBoard(log_dir='../tensorboard/' + gru_runtime_name, histogram_freq=0, batch_size=gru_batch_size, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize model\n",
    "# k.utils.plot_model(regressorGRU, to_file='../visualizations/' + gru_runtime_name +'.png', show_shapes=True)\n",
    "# IPython.display.Image('../visualizations/' + gru_runtime_name +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = regressorGRU.fit(x_train, y_train, epochs=gru_epochs, batch_size=gru_batch_size,  # shuffle=True,\n",
    "                        verbose=1, callbacks=[stopearly, tensorboard_gru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f20ca021ea3ce05f6c6a98db93775f1b2c9c022c"
   },
   "outputs": [],
   "source": [
    "# Preparing X_test and predicting the prices\n",
    "X_test = []\n",
    "for i in range(num_prev_elems, 311):\n",
    "    X_test.append(inputs[i - num_prev_elems:i, 0])\n",
    "# print(\"X_test after appendsc:\", X_test)\n",
    "X_test = np.array(X_test)\n",
    "# TODO: below: decode reshape\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "GRU_predicted_stock_price = regressorGRU.predict(X_test)\n",
    "GRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da8e9fa28510aa03e7dd06d5070d7b16e05ebb6e"
   },
   "outputs": [],
   "source": [
    "# Visualizing the results for GRU\n",
    "plot_predictions(test_set, GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23aec5ab1a717e3458c8d5cae68db0e7add091ae"
   },
   "outputs": [],
   "source": [
    "# Evaluating GRU\n",
    "return_rmse(test_set, GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3aa099ecc057f972525df4c7ef4209744d27106a"
   },
   "source": [
    "---\n",
    "## Sequence Generation (Siddarth)\n",
    "Here, I will generate a sequence using just initial (60) values instead of using last (60) values for every new prediction. **Due to doubts in various comments about predictions making use of test set values, I have decided to include sequence generation.** The above models make use of test set so it is using last (60) true values for predicting the new value(I will call it a benchmark). This is why the error is so low. Strong models can bring similar results like above models for sequences too but they require more than just data which has previous values. In case of stocks, we need to know the sentiments of the market, the movement of other stocks and a lot more. So, don't expect a remotely accurate plot. The error will be great and the best I can do is generate the trend similar to the test set. A GRU model is used for predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2949de3c668a788117221000567aaa7033cd1a17"
   },
   "outputs": [],
   "source": [
    "# Preparing sequence data\n",
    "initial_sequence = x_train[2708,:]  # TODO: how did this value of 2708 get generated?\n",
    "sequence = []\n",
    "for i in range(251):  # TODO: how did this value of 251 get generated?\n",
    "    new_prediction = regressorGRU.predict(\n",
    "        initial_sequence.reshape(initial_sequence.shape[1],initial_sequence.shape[0],1)\n",
    "    )\n",
    "    initial_sequence = initial_sequence[1:]\n",
    "    initial_sequence = np.append(initial_sequence,new_prediction,axis=0)\n",
    "    sequence.append(new_prediction)\n",
    "sequence = sc.inverse_transform(np.array(sequence).reshape(251,1))  # What does the array look like before reshape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be390cade0cf4b31cb99e069570e1be2c5594c8b"
   },
   "outputs": [],
   "source": [
    "# Visualizing the sequence\n",
    "plot_predictions(test_set,sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59095a00a05a68b97d76fc13bb2d618c337e1c80"
   },
   "outputs": [],
   "source": [
    "# Evaluating the sequence\n",
    "return_rmse(test_set,sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1003e9a17b23c1e3c7b7298250f8b071a93ae91"
   },
   "source": [
    "So, GRU works better than LSTM in this case. Bidirectional LSTM is also a good way so make the model stronger. But this may vary for different data sets. **Applying both LSTM and GRU together gave even better results.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
